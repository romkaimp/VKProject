{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T10:39:50.857231Z",
     "start_time": "2025-04-17T10:39:50.854072Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchaudio.datasets as datasets\n",
    "from torchaudio.transforms import RNNTLoss\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.functional import rnnt_loss\n",
    "import os\n",
    "import glob\n",
    "import pickle as pkl\n",
    "from LM.tokenizer.BPE import tokenize, tokenizer\n",
    "from LM.transformer import TransformerLanguageModel\n",
    "from AudioModel import ConformerEncoder, CTCConformer\n",
    "from dataset_SpecAugment import MyPipeline, LibriSpeechAugmented, LibriSpeechWav2Vec\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:39:51.681689Z",
     "start_time": "2025-04-17T10:39:51.677323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "merges_path = os.path.join(current_dir, \"LM\\\\tokenizer\", \"merges.pkl\")\n",
    "vocab_path = os.path.join(current_dir, \"LM\\\\tokenizer\", \"vocabulary.pkl\")\n",
    "\n",
    "# Загрузка merges.pkl\n",
    "with open(merges_path, \"rb\") as f:\n",
    "    merges = pkl.load(f)\n",
    "    print(\"Загрузка merges.pkl успешна\")\n",
    "\n",
    "# Загрузка vocab.pkl\n",
    "with open(vocab_path, \"rb\") as f:\n",
    "    vocab = pkl.load(f)\n",
    "    print(\"Загрузка vocabulary.pkl успешна\")\n",
    "\n",
    "#vocab[0] = '<|blank|>'\n",
    "#vocab[1] = '<|padding|>'\n",
    "#vocab[2] = '<|startoftext|>'\n",
    "vocab[1] = '<|blank|>'\n",
    "\n",
    "token_to_id = {vocab[i]: i for i in range(len(vocab))}\n",
    "id_to_token = {i: vocab[i] for i in range(len(vocab))}\n",
    "PAD_ID = 2\n",
    "BLANK_ID = 1\n",
    "START_ID = 0\n",
    "print(vocab)"
   ],
   "id": "b724a196e53631a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка merges.pkl успешна\n",
      "Загрузка vocabulary.pkl успешна\n",
      "['<|startoftext|>', '<|blank|>', '<|padding|>', \"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'Ġ', 'ĠT', 'HE', 'ĠA', 'IN', 'ĠTHE', 'ĠW', 'ĠS', 'ĠO', 'RE', 'ND', 'ĠH', 'ER', 'ĠB', 'ĠM', 'OU', 'IT', 'ĠF', 'IS', 'ĠC', 'AT', 'ED', 'ĠAND', 'ĠOF', 'EN', 'ON', 'ING', 'ĠTO', 'ĠP', 'OR', 'ES', 'ĠD', 'ĠTH', 'ĠL', 'AN', 'AS', 'ĠIN', 'AR', 'LL', 'ĠN', 'ĠHE', 'ĠG', 'AD', 'LE', 'OM', 'ĠE', 'ĠBE', 'OT', 'UT', 'IC', 'OW', 'LY', 'SE', 'ĠI', 'ST', 'VE', 'ĠWAS', 'LD', 'ĠWH', 'GH', 'ĠIT', 'ĠTHAT', 'ĠON', 'ĠU', 'ENT', 'AL', 'THE', 'ID', 'IM', 'VER', 'ĠHIS', 'ĠY', 'ĠRE', 'IR', 'ITH', 'CE', 'ION', 'ĠR', 'ĠWITH', 'ĠWE', 'ET', 'ĠAS', 'ĠFOR', 'AY', 'ĠST', 'UR', 'ĠHAD', 'GHT', 'ĠYOU', 'OO', 'ĠNOT', 'TER', 'ĠAN', 'AND', 'AC', 'ĠIS', 'ĠAT', 'ĠSE']\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:39:51.694468Z",
     "start_time": "2025-04-17T10:39:51.683685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'depthwise_conv_kernel_size':3,\n",
    "    'num_heads':8,\n",
    "    'num_conformers':4,\n",
    "    'mask_ratio':0.05,\n",
    "    'distraction_ratio':0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 1,\n",
    "    'epochs': 128,\n",
    "    'input_dim': 128,\n",
    "    'embedding_dim': 64,\n",
    "    'temperature':0.5,\n",
    "    'dataset': \"LibriSpeech dev-clean\",\n",
    "    'vocab_size': len(vocab),\n",
    "    'decoder_dim': 64,\n",
    "    'weight_decay': 1e-5,\n",
    "}\n",
    "LMconfig = {\n",
    "    'dim_feedforward': 64,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 8,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 256,\n",
    "    'embedding_dim': 64,\n",
    "    'dataset': \"LibriSpeech dev-clean\",\n",
    "    'vocab_size': len(vocab),\n",
    "}"
   ],
   "id": "740046be3be21626",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:39:52.630287Z",
     "start_time": "2025-04-17T10:39:52.561730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "modelEnc = ConformerEncoder(input_dim=config[\"input_dim\"],\n",
    "                embed_dim=config['embedding_dim'],\n",
    "                ffn_dim=config['embedding_dim'],\n",
    "                depthwise_conv_kernel_size=config['depthwise_conv_kernel_size'],\n",
    "                num_heads=config['num_heads'],\n",
    "                num_layers=config['num_conformers'],\n",
    "                )\n",
    "model = CTCConformer(modelEnc, config[\"vocab_size\"])\n",
    "model_path = os.path.join(\"best_models/conformerctc\", \"best_model_2.7741.pt\")\n",
    "pretrained_state_dict = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "LMmodel = TransformerLanguageModel(LMconfig['vocab_size'],\n",
    "                                 LMconfig['embedding_dim'],\n",
    "                                 LMconfig['num_heads'],\n",
    "                                 LMconfig['dim_feedforward'],\n",
    "                                 LMconfig['num_layers']\n",
    "                                 ).to(device)\n",
    "model_pattern = os.path.join(\"best_models/transformer\", \"model_244_*\")\n",
    "model_files = glob.glob(model_pattern)\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(f\"Файл не найден по шаблону: {model_pattern}\")\n",
    "model_path = model_files[0]\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "LMmodel.load_state_dict(checkpoint)"
   ],
   "id": "c9ebc00feef0b369",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "7c74a602523c8343"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:40:05.753311Z",
     "start_time": "2025-04-17T10:39:53.422904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Parsing librispeech\n",
    "data = datasets.LIBRISPEECH(\"data\", url=\"dev-clean\", )\n",
    "samples = []\n",
    "lens = []\n",
    "corpus = []\n",
    "for i in range(2800):\n",
    "    try:\n",
    "        samples.append(data.__getitem__(i))\n",
    "        lens.append(data.__getitem__(i)[0].shape[1])\n",
    "        corpus.append(list(map(lambda x: token_to_id[x], [vocab[START_ID]] + tokenize(data.__getitem__(i)[2], merges))))\n",
    "    except IndexError as err:\n",
    "        break\n",
    "\n",
    "num = len(samples)\n",
    "\n",
    "print(\"Total num of .flac:\", num)\n",
    "print(\"Max len and min len of .flac (sec * 16000)\", max(lens), min(lens))\n",
    "plt.hist(lens, density=True)\n",
    "plt.show()\n",
    "arr = np.array(lens)\n",
    "\n",
    "max_length = max(len(seq) for seq in corpus)\n",
    "\n",
    "class MixedDataset(Dataset):\n",
    "    def __init__(self, audio_dataset: Dataset, text_corpus: list, max_len: int):\n",
    "        self.audio_dataset = audio_dataset\n",
    "        self.text_corpus = text_corpus\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.text_corpus[idx], dtype=torch.int16)\n",
    "        sample = sample[:self.max_len]\n",
    "        length = sample.shape[-1]\n",
    "        padding = torch.ones((self.max_len - sample.shape[-1])) * PAD_ID\n",
    "        sample = torch.cat((sample, padding), dim=0)\n",
    "        return *self.audio_dataset.__getitem__(idx), torch.tensor(sample, dtype=torch.float), length\n",
    "\n",
    "audio_dataset = LibriSpeechWav2Vec(\"data\",\n",
    "                             url=\"dev-clean\",\n",
    "                             max_length=int(arr.mean()\n",
    "                                            + arr.std() * 3\n",
    "                                            ),\n",
    "                             n_mel=config['input_dim'],)\n",
    "\n",
    "dataset = MixedDataset(audio_dataset, corpus, max_length)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ],
   "id": "89c2ed52e0bc6e97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of .flac: 2703\n",
      "Max len and min len of .flac (sec * 16000) 522320 23120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test",
   "id": "79bd01745ad88b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:40:06.624036Z",
     "start_time": "2025-04-17T10:40:06.592964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, x_lengths, y, y_lengths = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    x_len = x_lengths.to(device)\n",
    "    y_len = y_lengths.to(device)\n",
    "\n",
    "    logits = model(x, x_len)  # ConformerCTC возвращает (logits, lengths)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    if i > 0:\n",
    "        break"
   ],
   "id": "3c66208953072e51",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_206916\\2694272469.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return *self.audio_dataset.__getitem__(idx), torch.tensor(sample, dtype=torch.float), length\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:40:07.451113Z",
     "start_time": "2025-04-17T10:40:07.448089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def length_to_mask(inputs, lengths, dtype=None):\n",
    "    batch_size = lengths.size(0)\n",
    "    seq_len = inputs.size(1) if isinstance(inputs, torch.Tensor) else inputs\n",
    "    # Causal mask [seq_len, seq_len]\n",
    "    tgt_mask = torch.triu(torch.ones((seq_len, seq_len), device=device)).transpose(0, 1)\n",
    "    tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
    "\n",
    "    # Padding mask [batch_size, seq_len]\n",
    "    key_padding_mask = (torch.arange(seq_len, device=device).expand(batch_size, seq_len) >= lengths.unsqueeze(1))\n",
    "\n",
    "    if dtype is not None:\n",
    "        key_padding_mask = key_padding_mask.to(dtype=dtype)\n",
    "\n",
    "    return tgt_mask, key_padding_mask"
   ],
   "id": "86c66ff80570cbb6",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:40:08.220599Z",
     "start_time": "2025-04-17T10:40:08.216276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "\n",
    "def shallow_fusion_beam_search(log_probs, lm_model, beam_width=5, lm_weight=0.2, blank_idx=1, eos_idx=1, device='cuda'):\n",
    "    \"\"\"\n",
    "    Shallow fusion of CTC and transformer LM using beam search decoding.\n",
    "\n",
    "    Args:\n",
    "        log_probs: Tensor (B, T, V) - log-probs from CTC (log_softmax already applied)\n",
    "        lm_model: language model (transformer), returns logits (not log_softmax)\n",
    "        beam_width: number of beams\n",
    "        lm_weight: weight of the LM in fusion\n",
    "        blank_idx: index of <blank> in CTC vocab (default 1)\n",
    "        eos_idx: index of <eos> in LM vocab (default 1)\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "    Returns:\n",
    "        List of best sequences for each batch\n",
    "    \"\"\"\n",
    "\n",
    "    B, T, V = log_probs.shape\n",
    "    results = []\n",
    "\n",
    "    for b in range(B):\n",
    "        beams = [( tuple(), 0.0)]\n",
    "\n",
    "        for t in range(T):\n",
    "            timestep_probs = log_probs[b, t]\n",
    "            top_token = torch.argmax(timestep_probs).item()\n",
    "\n",
    "            if top_token == blank_idx:\n",
    "                continue\n",
    "\n",
    "            new_beams = defaultdict(lambda: -float(\"inf\"))\n",
    "            for prefix, score in beams:\n",
    "                for v in range(V):\n",
    "                    prob = log_probs[b, t, v].item()\n",
    "\n",
    "                    if v == blank_idx:\n",
    "                        # Stay with current prefix\n",
    "                        new_beams[prefix] = max(new_beams[prefix], score + prob)\n",
    "                    else:\n",
    "                        new_prefix = prefix + (v,)\n",
    "\n",
    "                        if len(prefix) > 0:\n",
    "                            lm_input = torch.tensor([new_prefix], device=device)\n",
    "\n",
    "                            lengths_tensor = torch.tensor([len(new_prefix)]).to(device)\n",
    "\n",
    "                            tgt_mask, tgt_key_padding_mask = length_to_mask(lm_input, lengths_tensor)\n",
    "                            tgt_mask, tgt_key_padding_mask = tgt_mask.to(device), tgt_key_padding_mask.to(device)\n",
    "\n",
    "                            lm_logits = lm_model(lm_input, tgt_mask=tgt_mask, lengths=lengths_tensor, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "                            lm_logits = lm_logits[0, -1, :]\n",
    "                            lm_logprob = F.log_softmax(lm_logits, dim=-1)[v].item()\n",
    "                        else:\n",
    "                            lm_logprob = 0.0\n",
    "\n",
    "                        fused_score = score + prob + lm_weight * lm_logprob\n",
    "                        new_beams[new_prefix] = max(new_beams[new_prefix], fused_score)\n",
    "\n",
    "                        # Select top-K beams\n",
    "                        beams = heapq.nlargest(beam_width, new_beams.items(), key=lambda x: x[1])\n",
    "\n",
    "        best_hypo = max(beams, key=lambda x: x[1])[0]\n",
    "        results.append(best_hypo)\n",
    "\n",
    "    return results"
   ],
   "id": "1f8e790c8312f304",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:42:18.297290Z",
     "start_time": "2025-04-17T10:41:40.327207Z"
    }
   },
   "cell_type": "code",
   "source": "shal_res = shallow_fusion_beam_search(log_probs, lm_model=LMmodel)",
   "id": "5f8c60bfc392eab",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:42:36.480679Z",
     "start_time": "2025-04-17T10:42:36.415477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(shal_res)\n",
    "print(y)"
   ],
   "id": "632865817bae99e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 49, 35, 78)]\n",
      "tensor([[  0.,  17.,  59., 125.,  44.,  48., 121.,  30.,  20.,  24.,  12.,  15.,\n",
      "         121.,   3.,  22.,  44.,  64.,  17.,  42.,  63.,  60.,  22.,  66.,  23.,\n",
      "           8.,  39.,  84.,  56.,  62.,  64., 100.,  44.,  50., 121.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,\n",
      "           2.,   2.,   2.,   2.,   2.]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:44:35.768969Z",
     "start_time": "2025-04-17T10:44:35.707581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = []\n",
    "for b in range(log_probs.shape[0]):\n",
    "    for t in range(log_probs.shape[1]):\n",
    "        if torch.argmax(log_probs[b, t]).item() == 1:\n",
    "            continue\n",
    "        else:\n",
    "            res.append(id_to_token[torch.argmax(log_probs[b, t]).item()])\n",
    "res = ''.join(res[1:]).replace(\"Ġ\", \" \")\n",
    "print(res)\n",
    "\n",
    "sh_res = []\n",
    "for b in range(len(shal_res)):\n",
    "    for t in range(len(shal_res[b])):\n",
    "        if shal_res[b][t] == 1 or shal_res[b][t] == 2:\n",
    "            continue\n",
    "        else:\n",
    "            sh_res.append(id_to_token[shal_res[b][t]])\n",
    "sh_res = ''.join(sh_res[1:]).replace(\"Ġ\", \" \")\n",
    "print(sh_res)\n",
    "\n",
    "true_res = []\n",
    "for b in range(y.shape[0]):\n",
    "    for t in range(y.shape[1]):\n",
    "        if y[b, t].item() == 1 or y[b, t].item() == 2:\n",
    "            continue\n",
    "        else:\n",
    "            true_res.append(id_to_token[y[b, t].item()])\n",
    "true_res = ''.join(true_res[1:]).replace(\"Ġ\", \" \")\n",
    "print(true_res)"
   ],
   "id": "711fb4d00add7ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C THEINGUT B AND HE HIS MIER\n",
      " C THEUT\n",
      "NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ad093d071b52154"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
