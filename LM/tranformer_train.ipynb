{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-14T21:28:58.366484Z",
     "start_time": "2025-04-14T21:28:54.446808Z"
    }
   },
   "source": [
    "from tokenizer.BPE import tokenize, tokenizer\n",
    "import pickle as pkl\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchaudio.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformer import TransformerLanguageModel\n",
    "import wandb\n",
    "from datetime import datetime"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PyCharmPrj\\ML\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenized dataset creation",
   "id": "3a1d16195e2b1453"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:28:58.379029Z",
     "start_time": "2025-04-14T21:28:58.375715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "merges_path = os.path.join(current_dir, \"tokenizer\", \"merges.pkl\")\n",
    "vocab_path = os.path.join(current_dir, \"tokenizer\", \"vocabulary.pkl\")\n",
    "\n",
    "# Загрузка merges.pkl\n",
    "with open(merges_path, \"rb\") as f:\n",
    "    merges = pkl.load(f)\n",
    "    print(\"Загрузка merges.pkl успешна\")\n",
    "\n",
    "# Загрузка vocab.pkl\n",
    "with open(vocab_path, \"rb\") as f:\n",
    "    vocab = pkl.load(f)\n",
    "    print(\"Загрузка vocabulary.pkl успешна\")\n",
    "\n",
    "text = 'HELLO MY NAME IS BILL'\n",
    "tokens = [vocab[0]] + tokenize(text, merges) + [vocab[1]]\n",
    "#print(tokens)\n",
    "#print(tokenizer.convert_tokens_to_string(tokens))"
   ],
   "id": "d8896b53892a218d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка merges.pkl успешна\n",
      "Загрузка vocabulary.pkl успешна\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:29:03.882679Z",
     "start_time": "2025-04-14T21:28:58.461252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wandb.init(project='TransformerLM')\n",
    "config = {\n",
    "    'dim_feedforward': 64,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 8,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 256,\n",
    "    'embedding_dim': 64,\n",
    "    'dataset': \"LibriSpeech dev-clean\",\n",
    "    'vocab_size': len(vocab),\n",
    "}\n",
    "wandb.config.update(config)"
   ],
   "id": "3539065f3d01bd41",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: roman-kuznetsov (roman-kuznetsov-bmstu-) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>D:\\PyCharmPrj\\VKProject\\LM\\wandb\\run-20250415_002903-7q3jcr9p</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM/runs/7q3jcr9p' target=\"_blank\">electric-snowflake-23</a></strong> to <a href='https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM' target=\"_blank\">https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM/runs/7q3jcr9p' target=\"_blank\">https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM/runs/7q3jcr9p</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data mining",
   "id": "b1565f82956466ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:29:07.836901Z",
     "start_time": "2025-04-14T21:29:03.892528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_to_id = {vocab[i]: i for i in range(len(vocab))}\n",
    "id_to_token = {i: vocab[i] for i in range(len(vocab))}\n",
    "PAD_ID = 2\n",
    "\n",
    "data = datasets.LIBRISPEECH(\"../data\", url=\"dev-clean\", )\n",
    "corpus = []\n",
    "for i in range(2800):\n",
    "    try:\n",
    "        corpus.append(list(map(lambda x: token_to_id[x], [vocab[0]] + tokenize(data.__getitem__(i)[2], merges) + [vocab[1]])))\n",
    "    except IndexError as err:\n",
    "        break\n",
    "\n",
    "max_length = max(len(seq) for seq in corpus)\n",
    "print(max_length)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, max_len):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.data[idx], dtype=torch.int16)\n",
    "        sample = sample[:self.max_len]\n",
    "        length = sample.shape[-1]\n",
    "        padding = torch.ones((self.max_len - sample.shape[-1])) * 2\n",
    "        sample = torch.cat((sample, padding), dim=0)\n",
    "        return torch.tensor(sample, dtype=torch.float), length\n",
    "\n",
    "dataset = TextDataset(corpus, max_length)\n",
    "\n",
    "train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2)\n",
    "\n",
    "# Создание тренировочного и валидационного датасетов\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Создание DataLoader-ов для тренировочного и валидационного датасетов\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])"
   ],
   "id": "3162e102f1989c80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:29:07.851046Z",
     "start_time": "2025-04-14T21:29:07.847276Z"
    }
   },
   "cell_type": "code",
   "source": "print(token_to_id)",
   "id": "db0c5c71aaa92007",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<|startoftext|>': 0, '<|endoftext|>': 1, '<|padding|>': 2, \"'\": 3, 'A': 4, 'B': 5, 'C': 6, 'D': 7, 'E': 8, 'F': 9, 'G': 10, 'H': 11, 'I': 12, 'J': 13, 'K': 14, 'L': 15, 'M': 16, 'N': 17, 'O': 18, 'P': 19, 'Q': 20, 'R': 21, 'S': 22, 'T': 23, 'U': 24, 'V': 25, 'W': 26, 'X': 27, 'Y': 28, 'Z': 29, 'Ġ': 30, 'ĠT': 31, 'HE': 32, 'ĠA': 33, 'IN': 34, 'ĠTHE': 35, 'ĠW': 36, 'ĠS': 37, 'ĠO': 38, 'RE': 39, 'ND': 40, 'ĠH': 41, 'ER': 42, 'ĠB': 43, 'ĠM': 44, 'OU': 45, 'IT': 46, 'ĠF': 47, 'IS': 48, 'ĠC': 49, 'AT': 50, 'ED': 51, 'ĠAND': 52, 'ĠOF': 53, 'EN': 54, 'ON': 55, 'ING': 56, 'ĠTO': 57, 'ĠP': 58, 'OR': 59, 'ES': 60, 'ĠD': 61, 'ĠTH': 62, 'ĠL': 63, 'AN': 64, 'AS': 65, 'ĠIN': 66, 'AR': 67, 'LL': 68, 'ĠN': 69, 'ĠHE': 70, 'ĠG': 71, 'AD': 72, 'LE': 73, 'OM': 74, 'ĠE': 75, 'ĠBE': 76, 'OT': 77, 'UT': 78, 'IC': 79, 'OW': 80, 'LY': 81, 'SE': 82, 'ĠI': 83, 'ST': 84, 'VE': 85, 'ĠWAS': 86, 'LD': 87, 'ĠWH': 88, 'GH': 89, 'ĠIT': 90, 'ĠTHAT': 91, 'ĠON': 92, 'ĠU': 93, 'ENT': 94, 'AL': 95, 'THE': 96, 'ID': 97, 'IM': 98, 'VER': 99, 'ĠHIS': 100, 'ĠY': 101, 'ĠRE': 102, 'IR': 103, 'ITH': 104, 'CE': 105, 'ION': 106, 'ĠR': 107, 'ĠWITH': 108, 'ĠWE': 109, 'ET': 110, 'ĠAS': 111, 'ĠFOR': 112, 'AY': 113, 'ĠST': 114, 'UR': 115, 'ĠHAD': 116, 'GHT': 117, 'ĠYOU': 118, 'OO': 119, 'ĠNOT': 120, 'TER': 121, 'ĠAN': 122, 'AND': 123, 'AC': 124, 'ĠIS': 125, 'ĠAT': 126, 'ĠSE': 127}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:29:07.878694Z",
     "start_time": "2025-04-14T21:29:07.860433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, el in enumerate(val_loader):\n",
    "    if i < 4:\n",
    "        print(el)\n",
    "    else:\n",
    "        break"
   ],
   "id": "261643c91b8b8bb5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_72876\\942115274.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float), length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  0.,  67.,   7.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  59.,  12.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  96.,  61.,  ...,   2.,   2.,   2.],\n",
      "        ...,\n",
      "        [  0.,  32.,  86.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  32., 116.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  12.,  57.,  ...,   2.,   2.,   2.]]), tensor([ 83,  83,  57,  19, 113,  29,  47,  45,  47,  24,  33,  37,  19,  21,\n",
      "         35, 117,  48, 148,  94, 170,  59,  34, 127,  23,  47,  15,  15,  67,\n",
      "         44,  79,  22,  50,  45,  53,  21,  36, 128,  46, 196,  53,  51,  87,\n",
      "         56,  46, 140,  50,  14,  85,  29,  28,  53,  76, 154,  12, 102,  32,\n",
      "         58,  69,  40,  66,  49,  17,  74,  13])]\n",
      "[tensor([[  0.,   5.,  28.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  50.,  35.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  96.,  39.,  ...,   2.,   2.,   2.],\n",
      "        ...,\n",
      "        [  0.,  55.,  44.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,   5.,   8.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  12., 116.,  ...,   2.,   2.,   2.]]), tensor([135, 100,  97,  42,  41, 111,  77,  43,  43, 163,  35,  21,  38,  38,\n",
      "         53,  23,  24, 102,  74,  46,  17,  11,  39, 100,  44,  36,  18,  30,\n",
      "         18,  21,  15,  93,  25,  33, 134,  59,  71,  71,  66,  22, 139,  22,\n",
      "         12,  72,  86,  62,  91, 150,  28,  16,  26,  28,  85, 212, 114,  33,\n",
      "         31,  75,  31,  47,  13,  16,  22,  70])]\n",
      "[tensor([[  0.,  18.,  11.,  ...,   2.,   2.,   2.],\n",
      "        [  0., 123.,  37.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  28.,  45.,  ...,   2.,   2.,   2.],\n",
      "        ...,\n",
      "        [  0.,  96.,  36.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,   5.,  78.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  12., 111.,  ...,   2.,   2.,   2.]]), tensor([ 23,  18,  32,  90,  46,  20,  37,  70,  72, 142,  64,  40,  17,   7,\n",
      "         32,  34,  35,  56,  68,  52,  16,  33,  46,  99,  33,  56,  22,  70,\n",
      "        117,  22,  81, 136, 251,  81,  29,  31,  46, 112,  56,  95,  20, 104,\n",
      "         27, 107,  23,  35,  46,  81,  57,  29, 146,  73,  46,  69,  21,  69,\n",
      "        138,  97,  97,  31,  26,  58,  38,  28])]\n",
      "[tensor([[  0.,  96.,  37.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  12., 127.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  18.,   9.,  ...,   2.,   2.,   2.],\n",
      "        ...,\n",
      "        [  0.,  15.,  74.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  96.,  49.,  ...,   2.,   2.,   2.],\n",
      "        [  0.,  96.,  47.,  ...,   2.,   2.,   2.]]), tensor([ 30, 157,  61,  22,  46,  35,  30, 126,  95,  40,  35,  48, 107,  31,\n",
      "         21, 112,  39,  12,  75,  47,  27,  43,  52,  99,  13,  49,  19,  26,\n",
      "         75, 119,  36,  26, 158,  29,  17,  51,  52, 108,  37,  12,  75,  69,\n",
      "         44,  70,  24,  43, 136,  28,  27,  29,  60, 168, 145,  70,  73,  26,\n",
      "         81,  72,  86,  26,  25,  13, 136,  66])]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### prerequisites",
   "id": "8cc78ceefebf9928"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:29:07.892359Z",
     "start_time": "2025-04-14T21:29:07.889220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def length_to_mask(inputs, lengths, dtype=None):\n",
    "    batch_size = lengths.size(0)\n",
    "    seq_len = inputs.size(1) if isinstance(inputs, torch.Tensor) else inputs\n",
    "    # Causal mask [seq_len, seq_len]\n",
    "    tgt_mask = torch.triu(torch.ones((seq_len, seq_len), device=device)).transpose(0, 1)\n",
    "    tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
    "\n",
    "    # Padding mask [batch_size, seq_len]\n",
    "    key_padding_mask = (torch.arange(seq_len, device=device).expand(batch_size, seq_len) >= lengths.unsqueeze(1))\n",
    "\n",
    "    if dtype is not None:\n",
    "        key_padding_mask = key_padding_mask.to(dtype=dtype)\n",
    "\n",
    "    return tgt_mask, key_padding_mask"
   ],
   "id": "be73f2ce060556f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:29:08.048874Z",
     "start_time": "2025-04-14T21:29:07.902911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Параметры модели\n",
    "vocab_size = config['vocab_size']\n",
    "embedding_dim = config['embedding_dim']\n",
    "dim_feedforward = config['dim_feedforward']\n",
    "num_heads = config['num_heads']\n",
    "num_layers = config['num_layers']\n",
    "num_epochs = config['epochs']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size, embedding_dim, num_heads, dim_feedforward, num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=PAD_ID)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "warmup_steps = 0.2 * total_steps\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ],
   "id": "685d78a0975ce242",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training functions",
   "id": "af2f053bfe134109"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:29:08.068399Z",
     "start_time": "2025-04-14T21:29:08.061811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(epoch_index, model, training_loader, scheduler, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    total_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (data, lengths) in enumerate(training_loader):\n",
    "        input_data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        lengths = lengths.to(device) - 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = input_data[:, :-1].long()\n",
    "        targets = input_data[:, 1:].long()\n",
    "\n",
    "        seq_len = input_ids.size(1)\n",
    "        tgt_mask, tgt_key_padding_mask = length_to_mask(seq_len, lengths)\n",
    "        tgt_mask, tgt_key_padding_mask = tgt_mask.to(device), tgt_key_padding_mask.to(device)\n",
    "\n",
    "        outputs = model(input_ids, tgt_mask=tgt_mask, lengths=lengths, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1))  # [B * T, vocab_size]\n",
    "        targets = targets.reshape(-1)                    # [B * T]\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        mask = (targets != PAD_ID)\n",
    "        correct += (preds[mask] == targets[mask]).sum().item()\n",
    "        total += mask.sum().item()\n",
    "\n",
    "        # Logging\n",
    "        if i % 10 == 9:\n",
    "            wandb.log({'Loss': running_loss / 10, \"Train\": epoch_index * len(training_loader) + i + 1})\n",
    "            running_loss = 0.\n",
    "\n",
    "    acc = correct / total\n",
    "    wandb.log({'Accuracy': acc, \"Train\": epoch_index + 1})\n",
    "    return total_loss / len(training_loader)\n",
    "\n",
    "\n",
    "def validation(val_dataloader):\n",
    "    val_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_data, lengths in val_dataloader:\n",
    "            input_data = input_data.to(device)\n",
    "            lengths = lengths.to(device) - 1\n",
    "\n",
    "            input_ids = input_data[:, :-1].long()\n",
    "            targets = input_data[:, 1:].long()\n",
    "\n",
    "            tgt_mask, tgt_key_padding_mask = length_to_mask(input_ids, lengths)\n",
    "            tgt_mask, tgt_key_padding_mask = tgt_mask.to(device), tgt_key_padding_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids, tgt_mask=tgt_mask, lengths=lengths, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            mask = (targets != PAD_ID)\n",
    "            correct += (preds[mask] == targets[mask]).sum().item()\n",
    "            total += mask.sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    acc = correct / total\n",
    "    print(\"VAL LOSS =\", val_loss)\n",
    "    return val_loss, acc"
   ],
   "id": "d1316ba572adb871",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "4be65266825c4f2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T21:39:58.179059Z",
     "start_time": "2025-04-14T21:29:08.080751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_vloss = 1_000_000.\n",
    "counter = 0\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for epoch_number in range(num_epochs):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    model.train()\n",
    "    train_loss = train_one_epoch(\n",
    "        epoch_index=epoch_number,\n",
    "        model=model,\n",
    "        training_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=criterion,)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_acc = validation(val_loader)\n",
    "\n",
    "    wandb.log({'Loss/valid': val_loss, 'Accuracy/valid': val_acc}, step=epoch_number + 1)\n",
    "\n",
    "    if val_loss < best_vloss:\n",
    "        best_vloss = val_loss\n",
    "        model_path = os.path.join(\"../best_models/transformer\", 'model_{}_{}'.format(epoch_number + 1, timestamp))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    else:\n",
    "        if counter > 10:\n",
    "            break\n",
    "\n",
    "wandb.finish()"
   ],
   "id": "1a8d6e22d387b3ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_72876\\942115274.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float), length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL LOSS = 5.030080530378553\n",
      "EPOCH 2:\n",
      "VAL LOSS = 4.98148250579834\n",
      "EPOCH 3:\n",
      "VAL LOSS = 4.911378383636475\n",
      "EPOCH 4:\n",
      "VAL LOSS = 4.832472483317058\n",
      "EPOCH 5:\n",
      "VAL LOSS = 4.764636993408203\n",
      "EPOCH 6:\n",
      "VAL LOSS = 4.714720620049371\n",
      "EPOCH 7:\n",
      "VAL LOSS = 4.679715580410427\n",
      "EPOCH 8:\n",
      "VAL LOSS = 4.655336380004883\n",
      "EPOCH 9:\n",
      "VAL LOSS = 4.638492266337077\n",
      "EPOCH 10:\n",
      "VAL LOSS = 4.626918474833171\n",
      "EPOCH 11:\n",
      "VAL LOSS = 4.619131459130181\n",
      "EPOCH 12:\n",
      "VAL LOSS = 4.61301162507799\n",
      "EPOCH 13:\n",
      "VAL LOSS = 4.607271194458008\n",
      "EPOCH 14:\n",
      "VAL LOSS = 4.600317213270399\n",
      "EPOCH 15:\n",
      "VAL LOSS = 4.588482909732395\n",
      "EPOCH 16:\n",
      "VAL LOSS = 4.568347507052952\n",
      "EPOCH 17:\n",
      "VAL LOSS = 4.531885676913792\n",
      "EPOCH 18:\n",
      "VAL LOSS = 4.477660126156277\n",
      "EPOCH 19:\n",
      "VAL LOSS = 4.413697878519694\n",
      "EPOCH 20:\n",
      "VAL LOSS = 4.353170606825087\n",
      "EPOCH 21:\n",
      "VAL LOSS = 4.298386944664849\n",
      "EPOCH 22:\n",
      "VAL LOSS = 4.249417304992676\n",
      "EPOCH 23:\n",
      "VAL LOSS = 4.20613612069024\n",
      "EPOCH 24:\n",
      "VAL LOSS = 4.166478739844428\n",
      "EPOCH 25:\n",
      "VAL LOSS = 4.1301601197984485\n",
      "EPOCH 26:\n",
      "VAL LOSS = 4.095744927724202\n",
      "EPOCH 27:\n",
      "VAL LOSS = 4.064550293816461\n",
      "EPOCH 28:\n",
      "VAL LOSS = 4.036029232872857\n",
      "EPOCH 29:\n",
      "VAL LOSS = 4.012067953745524\n",
      "EPOCH 30:\n",
      "VAL LOSS = 3.9891729089948864\n",
      "EPOCH 31:\n",
      "VAL LOSS = 3.9699917899237738\n",
      "EPOCH 32:\n",
      "VAL LOSS = 3.9507926305135093\n",
      "EPOCH 33:\n",
      "VAL LOSS = 3.933493905597263\n",
      "EPOCH 34:\n",
      "VAL LOSS = 3.9175699022081165\n",
      "EPOCH 35:\n",
      "VAL LOSS = 3.9036989212036133\n",
      "EPOCH 36:\n",
      "VAL LOSS = 3.889177083969116\n",
      "EPOCH 37:\n",
      "VAL LOSS = 3.8775733047061496\n",
      "EPOCH 38:\n",
      "VAL LOSS = 3.866158194012112\n",
      "EPOCH 39:\n",
      "VAL LOSS = 3.8542370001475015\n",
      "EPOCH 40:\n",
      "VAL LOSS = 3.8463523387908936\n",
      "EPOCH 41:\n",
      "VAL LOSS = 3.837237993876139\n",
      "EPOCH 42:\n",
      "VAL LOSS = 3.8270637724134655\n",
      "EPOCH 43:\n",
      "VAL LOSS = 3.8191011216905384\n",
      "EPOCH 44:\n",
      "VAL LOSS = 3.810536755455865\n",
      "EPOCH 45:\n",
      "VAL LOSS = 3.803309546576606\n",
      "EPOCH 46:\n",
      "VAL LOSS = 3.795716736051771\n",
      "EPOCH 47:\n",
      "VAL LOSS = 3.7893928951687283\n",
      "EPOCH 48:\n",
      "VAL LOSS = 3.783214807510376\n",
      "EPOCH 49:\n",
      "VAL LOSS = 3.77766662173801\n",
      "EPOCH 50:\n",
      "VAL LOSS = 3.7715978622436523\n",
      "EPOCH 51:\n",
      "VAL LOSS = 3.7662048869662814\n",
      "EPOCH 52:\n",
      "VAL LOSS = 3.7626365025838218\n",
      "EPOCH 53:\n",
      "VAL LOSS = 3.7560754352145724\n",
      "EPOCH 54:\n",
      "VAL LOSS = 3.751798815197415\n",
      "EPOCH 55:\n",
      "VAL LOSS = 3.7480776045057507\n",
      "EPOCH 56:\n",
      "VAL LOSS = 3.742686907450358\n",
      "EPOCH 57:\n",
      "VAL LOSS = 3.7384773890177407\n",
      "EPOCH 58:\n",
      "VAL LOSS = 3.732802841398451\n",
      "EPOCH 59:\n",
      "VAL LOSS = 3.733319123586019\n",
      "EPOCH 60:\n",
      "VAL LOSS = 3.7262267536587186\n",
      "EPOCH 61:\n",
      "VAL LOSS = 3.723196268081665\n",
      "EPOCH 62:\n",
      "VAL LOSS = 3.720201015472412\n",
      "EPOCH 63:\n",
      "VAL LOSS = 3.7148433526357016\n",
      "EPOCH 64:\n",
      "VAL LOSS = 3.7123400105370417\n",
      "EPOCH 65:\n",
      "VAL LOSS = 3.710149976942274\n",
      "EPOCH 66:\n",
      "VAL LOSS = 3.7063202063242593\n",
      "EPOCH 67:\n",
      "VAL LOSS = 3.702422989739312\n",
      "EPOCH 68:\n",
      "VAL LOSS = 3.7002723746829562\n",
      "EPOCH 69:\n",
      "VAL LOSS = 3.6963180700937905\n",
      "EPOCH 70:\n",
      "VAL LOSS = 3.69240058792962\n",
      "EPOCH 71:\n",
      "VAL LOSS = 3.69069857067532\n",
      "EPOCH 72:\n",
      "VAL LOSS = 3.6886858145395913\n",
      "EPOCH 73:\n",
      "VAL LOSS = 3.6852784951527915\n",
      "EPOCH 74:\n",
      "VAL LOSS = 3.6824656592475042\n",
      "EPOCH 75:\n",
      "VAL LOSS = 3.680771509806315\n",
      "EPOCH 76:\n",
      "VAL LOSS = 3.678809748755561\n",
      "EPOCH 77:\n",
      "VAL LOSS = 3.676471392313639\n",
      "EPOCH 78:\n",
      "VAL LOSS = 3.6735835870107016\n",
      "EPOCH 79:\n",
      "VAL LOSS = 3.6708087391323514\n",
      "EPOCH 80:\n",
      "VAL LOSS = 3.6682838598887124\n",
      "EPOCH 81:\n",
      "VAL LOSS = 3.666840155919393\n",
      "EPOCH 82:\n",
      "VAL LOSS = 3.662498156229655\n",
      "EPOCH 83:\n",
      "VAL LOSS = 3.664503812789917\n",
      "EPOCH 84:\n",
      "VAL LOSS = 3.6613319714864097\n",
      "EPOCH 85:\n",
      "VAL LOSS = 3.657907353507148\n",
      "EPOCH 86:\n",
      "VAL LOSS = 3.656162076526218\n",
      "EPOCH 87:\n",
      "VAL LOSS = 3.651188294092814\n",
      "EPOCH 88:\n",
      "VAL LOSS = 3.6503584649827747\n",
      "EPOCH 89:\n",
      "VAL LOSS = 3.6491637229919434\n",
      "EPOCH 90:\n",
      "VAL LOSS = 3.645679924223158\n",
      "EPOCH 91:\n",
      "VAL LOSS = 3.6440902286105685\n",
      "EPOCH 92:\n",
      "VAL LOSS = 3.64314775996738\n",
      "EPOCH 93:\n",
      "VAL LOSS = 3.6406239403618708\n",
      "EPOCH 94:\n",
      "VAL LOSS = 3.6397804419199624\n",
      "EPOCH 95:\n",
      "VAL LOSS = 3.6329879495832653\n",
      "EPOCH 96:\n",
      "VAL LOSS = 3.6320416397518582\n",
      "EPOCH 97:\n",
      "VAL LOSS = 3.6319604184892444\n",
      "EPOCH 98:\n",
      "VAL LOSS = 3.625866757498847\n",
      "EPOCH 99:\n",
      "VAL LOSS = 3.624712096320258\n",
      "EPOCH 100:\n",
      "VAL LOSS = 3.6255467202928333\n",
      "EPOCH 101:\n",
      "VAL LOSS = 3.6159723069932728\n",
      "EPOCH 102:\n",
      "VAL LOSS = 3.6158492830064564\n",
      "EPOCH 103:\n",
      "VAL LOSS = 3.6148600578308105\n",
      "EPOCH 104:\n",
      "VAL LOSS = 3.611796882417467\n",
      "EPOCH 105:\n",
      "VAL LOSS = 3.609640200932821\n",
      "EPOCH 106:\n",
      "VAL LOSS = 3.605385568406847\n",
      "EPOCH 107:\n",
      "VAL LOSS = 3.603134208255344\n",
      "EPOCH 108:\n",
      "VAL LOSS = 3.5962960720062256\n",
      "EPOCH 109:\n",
      "VAL LOSS = 3.5963992012871637\n",
      "EPOCH 110:\n",
      "VAL LOSS = 3.594126489427355\n",
      "EPOCH 111:\n",
      "VAL LOSS = 3.591672976811727\n",
      "EPOCH 112:\n",
      "VAL LOSS = 3.5876578754848905\n",
      "EPOCH 113:\n",
      "VAL LOSS = 3.5860676765441895\n",
      "EPOCH 114:\n",
      "VAL LOSS = 3.5830530060662165\n",
      "EPOCH 115:\n",
      "VAL LOSS = 3.5810844898223877\n",
      "EPOCH 116:\n",
      "VAL LOSS = 3.5778854423099093\n",
      "EPOCH 117:\n",
      "VAL LOSS = 3.576709959242079\n",
      "EPOCH 118:\n",
      "VAL LOSS = 3.5811857647365994\n",
      "EPOCH 119:\n",
      "VAL LOSS = 3.569947693083021\n",
      "EPOCH 120:\n",
      "VAL LOSS = 3.5706444581349692\n",
      "EPOCH 121:\n",
      "VAL LOSS = 3.5627886984083386\n",
      "EPOCH 122:\n",
      "VAL LOSS = 3.5641117890675864\n",
      "EPOCH 123:\n",
      "VAL LOSS = 3.5576788319481745\n",
      "EPOCH 124:\n",
      "VAL LOSS = 3.558482699924045\n",
      "EPOCH 125:\n",
      "VAL LOSS = 3.5567060311635337\n",
      "EPOCH 126:\n",
      "VAL LOSS = 3.5556449360317655\n",
      "EPOCH 127:\n",
      "VAL LOSS = 3.5500468677944608\n",
      "EPOCH 128:\n",
      "VAL LOSS = 3.5501575734880237\n",
      "EPOCH 129:\n",
      "VAL LOSS = 3.5459174580044217\n",
      "EPOCH 130:\n",
      "VAL LOSS = 3.5462159050835504\n",
      "EPOCH 131:\n",
      "VAL LOSS = 3.539584928088718\n",
      "EPOCH 132:\n",
      "VAL LOSS = 3.5410537984636097\n",
      "EPOCH 133:\n",
      "VAL LOSS = 3.538780132929484\n",
      "EPOCH 134:\n",
      "VAL LOSS = 3.534050014283922\n",
      "EPOCH 135:\n",
      "VAL LOSS = 3.5351893107096353\n",
      "EPOCH 136:\n",
      "VAL LOSS = 3.5335301293267145\n",
      "EPOCH 137:\n",
      "VAL LOSS = 3.530996084213257\n",
      "EPOCH 138:\n",
      "VAL LOSS = 3.5266853703392878\n",
      "EPOCH 139:\n",
      "VAL LOSS = 3.52634769015842\n",
      "EPOCH 140:\n",
      "VAL LOSS = 3.519624630610148\n",
      "EPOCH 141:\n",
      "VAL LOSS = 3.5213597880469427\n",
      "EPOCH 142:\n",
      "VAL LOSS = 3.519447512096829\n",
      "EPOCH 143:\n",
      "VAL LOSS = 3.51526673634847\n",
      "EPOCH 144:\n",
      "VAL LOSS = 3.5169798798031278\n",
      "EPOCH 145:\n",
      "VAL LOSS = 3.5132754378848605\n",
      "EPOCH 146:\n",
      "VAL LOSS = 3.5086701181199818\n",
      "EPOCH 147:\n",
      "VAL LOSS = 3.509299490186903\n",
      "EPOCH 148:\n",
      "VAL LOSS = 3.5108802848392062\n",
      "EPOCH 149:\n",
      "VAL LOSS = 3.5066625542110867\n",
      "EPOCH 150:\n",
      "VAL LOSS = 3.5070141421424017\n",
      "EPOCH 151:\n",
      "VAL LOSS = 3.5059018664889865\n",
      "EPOCH 152:\n",
      "VAL LOSS = 3.502332395977444\n",
      "EPOCH 153:\n",
      "VAL LOSS = 3.503176318274604\n",
      "EPOCH 154:\n",
      "VAL LOSS = 3.4990228282080755\n",
      "EPOCH 155:\n",
      "VAL LOSS = 3.5028816064198813\n",
      "EPOCH 156:\n",
      "VAL LOSS = 3.4968147012922497\n",
      "EPOCH 157:\n",
      "VAL LOSS = 3.4942467212677\n",
      "EPOCH 158:\n",
      "VAL LOSS = 3.497284624311659\n",
      "EPOCH 159:\n",
      "VAL LOSS = 3.492663436465793\n",
      "EPOCH 160:\n",
      "VAL LOSS = 3.4957901901668973\n",
      "EPOCH 161:\n",
      "VAL LOSS = 3.492372883690728\n",
      "EPOCH 162:\n",
      "VAL LOSS = 3.4931155840555825\n",
      "EPOCH 163:\n",
      "VAL LOSS = 3.4905410872565374\n",
      "EPOCH 164:\n",
      "VAL LOSS = 3.48854488796658\n",
      "EPOCH 165:\n",
      "VAL LOSS = 3.489735788769192\n",
      "EPOCH 166:\n",
      "VAL LOSS = 3.487604988945855\n",
      "EPOCH 167:\n",
      "VAL LOSS = 3.4847629600101047\n",
      "EPOCH 168:\n",
      "VAL LOSS = 3.486049360699124\n",
      "EPOCH 169:\n",
      "VAL LOSS = 3.482157733705309\n",
      "EPOCH 170:\n",
      "VAL LOSS = 3.484795914755927\n",
      "EPOCH 171:\n",
      "VAL LOSS = 3.4796462059020996\n",
      "EPOCH 172:\n",
      "VAL LOSS = 3.482252015007867\n",
      "EPOCH 173:\n",
      "VAL LOSS = 3.481929143269857\n",
      "EPOCH 174:\n",
      "VAL LOSS = 3.4811087449391684\n",
      "EPOCH 175:\n",
      "VAL LOSS = 3.476740254296197\n",
      "EPOCH 176:\n",
      "VAL LOSS = 3.479113499323527\n",
      "EPOCH 177:\n",
      "VAL LOSS = 3.478993442323473\n",
      "EPOCH 178:\n",
      "VAL LOSS = 3.479698737462362\n",
      "EPOCH 179:\n",
      "VAL LOSS = 3.476186407936944\n",
      "EPOCH 180:\n",
      "VAL LOSS = 3.4770833121405706\n",
      "EPOCH 181:\n",
      "VAL LOSS = 3.471564292907715\n",
      "EPOCH 182:\n",
      "VAL LOSS = 3.467688904868232\n",
      "EPOCH 183:\n",
      "VAL LOSS = 3.472257958518134\n",
      "EPOCH 184:\n",
      "VAL LOSS = 3.474683814578586\n",
      "EPOCH 185:\n",
      "VAL LOSS = 3.469094567828708\n",
      "EPOCH 186:\n",
      "VAL LOSS = 3.470998446146647\n",
      "EPOCH 187:\n",
      "VAL LOSS = 3.470348702536689\n",
      "EPOCH 188:\n",
      "VAL LOSS = 3.469028843773736\n",
      "EPOCH 189:\n",
      "VAL LOSS = 3.4650408956739636\n",
      "EPOCH 190:\n",
      "VAL LOSS = 3.469228082233005\n",
      "EPOCH 191:\n",
      "VAL LOSS = 3.4675674438476562\n",
      "EPOCH 192:\n",
      "VAL LOSS = 3.466934416029188\n",
      "EPOCH 193:\n",
      "VAL LOSS = 3.466271824306912\n",
      "EPOCH 194:\n",
      "VAL LOSS = 3.4691453774770102\n",
      "EPOCH 195:\n",
      "VAL LOSS = 3.4650364451938205\n",
      "EPOCH 196:\n",
      "VAL LOSS = 3.4662878778245716\n",
      "EPOCH 197:\n",
      "VAL LOSS = 3.4661170641581216\n",
      "EPOCH 198:\n",
      "VAL LOSS = 3.465123759375678\n",
      "EPOCH 199:\n",
      "VAL LOSS = 3.4620088736216226\n",
      "EPOCH 200:\n",
      "VAL LOSS = 3.4634362591637506\n",
      "EPOCH 201:\n",
      "VAL LOSS = 3.4624832736121283\n",
      "EPOCH 202:\n",
      "VAL LOSS = 3.4647917217678494\n",
      "EPOCH 203:\n",
      "VAL LOSS = 3.4621340963575573\n",
      "EPOCH 204:\n",
      "VAL LOSS = 3.461380508210924\n",
      "EPOCH 205:\n",
      "VAL LOSS = 3.4607563548617892\n",
      "EPOCH 206:\n",
      "VAL LOSS = 3.4634346697065563\n",
      "EPOCH 207:\n",
      "VAL LOSS = 3.45942923757765\n",
      "EPOCH 208:\n",
      "VAL LOSS = 3.461492829852634\n",
      "EPOCH 209:\n",
      "VAL LOSS = 3.4596672587924533\n",
      "EPOCH 210:\n",
      "VAL LOSS = 3.4593441486358643\n",
      "EPOCH 211:\n",
      "VAL LOSS = 3.459282053841485\n",
      "EPOCH 212:\n",
      "VAL LOSS = 3.461444960700141\n",
      "EPOCH 213:\n",
      "VAL LOSS = 3.46101615164015\n",
      "EPOCH 214:\n",
      "VAL LOSS = 3.460019667943319\n",
      "EPOCH 215:\n",
      "VAL LOSS = 3.4588917096455893\n",
      "EPOCH 216:\n",
      "VAL LOSS = 3.457302305433485\n",
      "EPOCH 217:\n",
      "VAL LOSS = 3.458196904924181\n",
      "EPOCH 218:\n",
      "VAL LOSS = 3.4585134983062744\n",
      "EPOCH 219:\n",
      "VAL LOSS = 3.457038402557373\n",
      "EPOCH 220:\n",
      "VAL LOSS = 3.457261085510254\n",
      "EPOCH 221:\n",
      "VAL LOSS = 3.4608983993530273\n",
      "EPOCH 222:\n",
      "VAL LOSS = 3.4571324984232583\n",
      "EPOCH 223:\n",
      "VAL LOSS = 3.4560159312354193\n",
      "EPOCH 224:\n",
      "VAL LOSS = 3.4560369385613336\n",
      "EPOCH 225:\n",
      "VAL LOSS = 3.4577967060936823\n",
      "EPOCH 226:\n",
      "VAL LOSS = 3.457280980216132\n",
      "EPOCH 227:\n",
      "VAL LOSS = 3.4553274313608804\n",
      "EPOCH 228:\n",
      "VAL LOSS = 3.4566937022738986\n",
      "EPOCH 229:\n",
      "VAL LOSS = 3.4539458486768932\n",
      "EPOCH 230:\n",
      "VAL LOSS = 3.4570117791493735\n",
      "EPOCH 231:\n",
      "VAL LOSS = 3.4538185596466064\n",
      "EPOCH 232:\n",
      "VAL LOSS = 3.4540506733788385\n",
      "EPOCH 233:\n",
      "VAL LOSS = 3.452019108666314\n",
      "EPOCH 234:\n",
      "VAL LOSS = 3.4557766914367676\n",
      "EPOCH 235:\n",
      "VAL LOSS = 3.45405822330051\n",
      "EPOCH 236:\n",
      "VAL LOSS = 3.453894403245714\n",
      "EPOCH 237:\n",
      "VAL LOSS = 3.453499846988254\n",
      "EPOCH 238:\n",
      "VAL LOSS = 3.4544513490464954\n",
      "EPOCH 239:\n",
      "VAL LOSS = 3.4544702900780573\n",
      "EPOCH 240:\n",
      "VAL LOSS = 3.4540397591061063\n",
      "EPOCH 241:\n",
      "VAL LOSS = 3.4529197216033936\n",
      "EPOCH 242:\n",
      "VAL LOSS = 3.453489833407932\n",
      "EPOCH 243:\n",
      "VAL LOSS = 3.454340272479587\n",
      "EPOCH 244:\n",
      "VAL LOSS = 3.451908588409424\n",
      "EPOCH 245:\n",
      "VAL LOSS = 3.453898641798231\n",
      "EPOCH 246:\n",
      "VAL LOSS = 3.453485647837321\n",
      "EPOCH 247:\n",
      "VAL LOSS = 3.4528842767079673\n",
      "EPOCH 248:\n",
      "VAL LOSS = 3.453978829913669\n",
      "EPOCH 249:\n",
      "VAL LOSS = 3.4524910979800754\n",
      "EPOCH 250:\n",
      "VAL LOSS = 3.453113900290595\n",
      "EPOCH 251:\n",
      "VAL LOSS = 3.4528640111287436\n",
      "EPOCH 252:\n",
      "VAL LOSS = 3.4524501694573297\n",
      "EPOCH 253:\n",
      "VAL LOSS = 3.4523137410481772\n",
      "EPOCH 254:\n",
      "VAL LOSS = 3.4523705111609564\n",
      "EPOCH 255:\n",
      "VAL LOSS = 3.452342801623874\n",
      "EPOCH 256:\n",
      "VAL LOSS = 3.4524540636274548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▁▁▂▂▂▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>Loss</td><td>█▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▁▁▄▄▄▄▁▅▅▅▁▅▅▆▆▆▁▁▁▇▇█▁██▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.2851</td></tr><tr><td>Loss</td><td>3.20238</td></tr><tr><td>Train</td><td>256</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-snowflake-23</strong> at: <a href='https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM/runs/7q3jcr9p' target=\"_blank\">https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM/runs/7q3jcr9p</a><br> View project at: <a href='https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM' target=\"_blank\">https://wandb.ai/roman-kuznetsov-bmstu-/TransformerLM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250415_002903-7q3jcr9p\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tried to log to step 256 that is less than the current step 1024. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test prompt",
   "id": "b8d07b771562ee68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T22:05:03.031447Z",
     "start_time": "2025-04-14T22:05:03.027275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 8\n",
    "base, b_l = train_dataset.__getitem__(i)[0].clone().detach().tolist(), train_dataset.__getitem__(i)[1]\n",
    "init_one = []\n",
    "for tok in base:\n",
    "    init_one.append(id_to_token[tok])\n",
    "init_one = ''.join(init_one[1:-1]).replace(\"Ġ\", \" \")\n",
    "print(init_one)\n"
   ],
   "id": "3ea92d38e81a4b42",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_72876\\942115274.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float), length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE CAME TO HER SIDE AND SHE GAVE HIM NO GREETING<|endoftext|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|>\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T22:05:10.973579Z",
     "start_time": "2025-04-14T22:05:10.948412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "input_data = train_dataset.__getitem__(i)[0].unsqueeze(0).to(device)\n",
    "lengths = train_dataset.__getitem__(i)[1]\n",
    "with torch.no_grad():\n",
    "    input_data = input_data.to(device)\n",
    "    lengths = (torch.tensor(lengths).to(device) - 1).unsqueeze(0).to(device)\n",
    "\n",
    "    input_ids = input_data[:, :-1].long()\n",
    "    targets = input_data[:, 1:].long()\n",
    "\n",
    "    tgt_mask, tgt_key_padding_mask = length_to_mask(input_ids, lengths)\n",
    "    tgt_mask, tgt_key_padding_mask = tgt_mask.to(device), tgt_key_padding_mask.to(device)\n",
    "\n",
    "    outputs = model(input_ids, tgt_mask=tgt_mask, lengths=lengths, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "    outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "    targets = targets.reshape(-1)\n",
    "\n",
    "    preds = outputs.argmax(dim=1)\n",
    "preds = preds.cpu().tolist()\n",
    "init_one = []\n",
    "for tok in preds:\n",
    "    init_one.append(id_to_token[tok])\n",
    "init_one = ''.join(init_one[1:-1]).replace(\"Ġ\", \" \")\n",
    "print(init_one)"
   ],
   "id": "4894b5ca6099de18",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_72876\\942115274.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float), length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WASOUME TO THERSEHEE AND SHE SOOVE THEIM TOO LOATING AIEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
